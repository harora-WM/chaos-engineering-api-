"""LLM client for AWS Bedrock operations"""
import json
import time
import boto3
from botocore.exceptions import ClientError
from typing import Tuple, Generator


class LLMClient:
    """Client for LLM operations using AWS Bedrock"""

    def __init__(self, model: str = "anthropic.claude-3-haiku-20240307-v1:0", region: str = "us-east-1"):
        self.model = model
        self.region = region
        self.max_retries = 3
        self.client = None

    def test_connection(self) -> Tuple[bool, str]:
        """Test connection to AWS Bedrock"""
        try:
            # Create Bedrock Runtime client
            self.client = boto3.client("bedrock-runtime", region_name=self.region)

            # Try a simple test call
            test_payload = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 10,
                "messages": [
                    {
                        "role": "user",
                        "content": [{"type": "text", "text": "Hi"}],
                    }
                ],
            }

            response = self.client.invoke_model(
                modelId=self.model,
                body=json.dumps(test_payload)
            )

            return True, f"✅ Connected to AWS Bedrock with model '{self.model}'"

        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == 'ResourceNotFoundException':
                return False, f"❌ Model '{self.model}' not found. Check model ID and region."
            elif error_code == 'AccessDeniedException':
                return False, f"❌ Access denied. Enable model access at https://console.aws.amazon.com/bedrock/home?#/modelaccess"
            else:
                return False, f"❌ AWS Error: {error_code} - {e.response['Error']['Message']}"
        except Exception as e:
            return False, f"❌ Failed to connect to AWS Bedrock: {str(e)}"

    def analyze_with_bedrock(self, prompt: str) -> str:
        """Generate analysis using AWS Bedrock with retries"""
        if not self.client:
            self.client = boto3.client("bedrock-runtime", region_name=self.region)

        for attempt in range(self.max_retries):
            try:
                # Format the request payload using Bedrock's native structure
                native_request = {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 16384,
                    "temperature": 0.2,
                    "messages": [
                        {
                            "role": "user",
                            "content": [{"type": "text", "text": prompt}],
                        }
                    ],
                }

                # Invoke the model
                response = self.client.invoke_model(
                    modelId=self.model,
                    body=json.dumps(native_request)
                )

                # Decode the response body
                model_response = json.loads(response["body"].read())

                # Extract and return the response text
                response_text = model_response["content"][0]["text"]
                return response_text

            except ClientError as e:
                error_msg = f"Bedrock error (attempt {attempt+1}): {e.response['Error']['Code']} - {e.response['Error']['Message']}"
                print(error_msg)
                if attempt < self.max_retries - 1:
                    time.sleep(1 * (attempt + 1))

            except Exception as e:
                error_msg = f"Bedrock connection failed (attempt {attempt+1}): {str(e)}"
                print(error_msg)
                if attempt < self.max_retries - 1:
                    time.sleep(1 * (attempt + 1))

        raise Exception(f"All {self.max_retries} attempts failed")

    def analyze_with_bedrock_streaming(self, prompt: str) -> Generator[str, None, None]:
        """Generate analysis using AWS Bedrock with streaming response.

        Yields text chunks as they are generated by the model.
        """
        if not self.client:
            self.client = boto3.client("bedrock-runtime", region_name=self.region)

        # Format the request payload using Bedrock's native structure
        native_request = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 16384,
            "temperature": 0.2,
            "messages": [
                {
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}],
                }
            ],
        }

        # Invoke the model with streaming response
        streaming_response = self.client.invoke_model_with_response_stream(
            modelId=self.model,
            body=json.dumps(native_request)
        )

        # Extract and yield the response text in real-time
        for event in streaming_response["body"]:
            chunk = json.loads(event["chunk"]["bytes"])
            if chunk["type"] == "content_block_delta":
                text = chunk["delta"].get("text", "")
                if text:
                    yield text

    def _get_system_prompt(self) -> str:
        """Get the system prompt for the LLM"""
        return """You are a Chaos Engineering SRE expert with 15 years of experience."""
