"""Chaos plan generator using OpenSearch and LLM"""
import json
import time
from typing import Dict, Tuple, Generator
from .opensearch_client import OpenSearchClient
from .llm_client import LLMClient


class ChaosPlanGenerator:
    """Main class for generating chaos plans"""

    def __init__(self, opensearch_client: OpenSearchClient, llm_client: LLMClient):
        self.os_client = opensearch_client
        self.llm_client = llm_client

    def generate_plan(self, index_name: str, index_data: Dict, analysis_options: Dict) -> Tuple[str, Dict]:
        """Generate chaos engineering plan"""

        metrics = {
            "start_time": time.time(),
            "success": False,
            "error": None
        }

        try:
            # Create prompt
            prompt = self._create_prompt(index_name, index_data, analysis_options)

            # Generate with LLM
            plan = self.llm_client.analyze_with_bedrock(prompt)

            if plan:
                metrics.update({
                    "success": True,
                    "end_time": time.time(),
                    "duration_seconds": time.time() - metrics["start_time"],
                    "plan_length": len(plan)
                })
                return plan, metrics
            else:
                metrics["error"] = "Empty response from LLM"
                return "", metrics

        except Exception as e:
            metrics["error"] = str(e)
            return "", metrics

    def generate_plan_streaming(self, index_name: str, index_data: Dict, analysis_options: Dict) -> Generator[str, None, None]:
        """Generate chaos engineering plan with streaming output.

        Yields text chunks as they are generated by the LLM.
        Returns a generator that yields text chunks.
        """
        # Create prompt
        prompt = self._create_prompt(index_name, index_data, analysis_options)

        # Generate with LLM using streaming
        yield from self.llm_client.analyze_with_bedrock_streaming(prompt)

    def _create_prompt(self, index_name: str, index_data: Dict, analysis_options: Dict) -> str:
        """Create prompt for LLM"""

        # Extract sample documents
        hits = index_data.get("documents", {}).get("hits", {}).get("hits", [])
        sample_docs = []

        for i, hit in enumerate(hits[:1000]):  # Only use first 1000 docs for LLM
            source = hit.get("_source", {})
            # Truncate long messages
            message = source.get("message") or source.get("log") or str(source)

            sample_docs.append({
                "doc": i + 1,
                "timestamp": source.get("@timestamp", source.get("timestamp", "N/A")),
                "message": message,
                "level": source.get("level", source.get("severity", "N/A")),
                "service": source.get("service", source.get("app", "N/A"))
            })

        # Build prompt
        prompt = f"""
# CHAOS ENGINEERING PLAN GENERATION

## INDEX INFORMATION:
- **Index Name:** {index_name}
- **Total Documents:** {index_data.get('total_hits', 0):,}
- **Query Time:** {index_data.get('took_ms', 0)} ms

## SAMPLE LOG DOCUMENTS (1000 out of {len(hits)} fetched):
```json
{json.dumps(sample_docs, indent=2)}

Focus Area: {analysis_options.get('focus', 'All')}

Include Security: {analysis_options.get('security', True)}

Include External Dependencies: {analysis_options.get('include_external', True)}
You are a Chaos Engineering SRE expert with 15 years of experience. Your task is to analyze the logs and provide all necessary output under 16384 tokens only:

Step 1 — Log & Topology Analysis

Analyze the provided kubernetes configuration file and determine weaknesses around reliability, scalability and security and provide remedial recommendations specific to the logs(pin point the issue exactly and tell technical remedies specefic to the pod configuration and show its each analysis)

Parse and analyze the given application logs.

From logs, generate a textual service topology that includes:

IP addresses, Port numbers, FQDNs, URLs and methods such as GET/PUT/POST/etc.

All internal service communications (pod ↔ pod, pod ↔ DataBases, service ↔ service, middleware interactions, storage interactions).

All external communications (to DNS, LDAP, MQ, APIs, 3rd-party services, interfacing systems).

Show dependencies and propagation paths (e.g., "Pod A → Service B → External DB").

Represent topology as a clear textual graph (priority). The output for this could be something along the lines of the following structure strictly (example):

Textual Service Topology:
text
1. Pod: wmuitestcontroller-7ddb4564f4-hhcxb (IP: Kubernetes-internal, Port: 9090)
│
├──→ Consul: wmconsul.default.svc.cluster.local (DNS: Kubernetes service)
│ └──→ Service discovery and configuration
│
├──→ Database: PostgreSQL (via SPRING_DATASOURCE_URL_UIT)
│ └──→ Database operations (Hibernate, Liquibase)
│
├──→ RabbitMQ: (via SPRING_RABBITMQ_HOST)
│ └──→ Message queueing for async tasks
│
├──→ OpenSearch: (via OPENSEARCH_URIS)
│ └──→ Logging and search operations
│
├──→ Minio/S3: (via STORAGE_ENDPOINT, STORAGE_REGION)
│ └──→ Storage for screenshots, videos, HAR files, APK/IPA, etc.
│ ├──→ Bucket: wm-ui-test-screenshots
│ ├──→ Bucket: wm-ui-test-videos
│ ├──→ Bucket: wm-ft-tmp
│ ├──→ Bucket: wm-ui-test-har
│ ├──→ Bucket: wm-ui-test-browser-logs
│ ├──→ Bucket: wm-ui-test-scripts
│ ├──→ Bucket: wm-ui-test-page-source
│ ├──→ Bucket: wm-ui-test-apk-ipa
│ ├──→ Bucket: wm-ui-test-resource
│ └──→ Bucket: wm-ui-test-scm-repository
│
├──→ Gateway: (via WM_GATEWAY_BASEURL)
│ └──→ API gateway for external communications
│
├──→ External: JIRA (DBSRootUAT.crt) - Certificate lookup failed
│
├──→ External: Device Farm (video sessions)
│ └──→ chrome-124-0-6367-118-6-* sessions
│
└──→ External: ALM (via wmMetaClient.getTicketCredentialDTOByCode)
└──→ Ticket credential lookup
Communication Patterns:
- HTTP/REST: Consul, Gateway, ALM, Device Farm
- JDBC: PostgreSQL
- AMQP: RabbitMQ
- S3 API: Minio/S3
- OpenSearch API: OpenSearch(As you can see from the example all the communication patterns are noted and clear )
Key Observations(All key observations from the observed pattterns and more fixes should be displayed)

Step 2 — Entity Identification

For each node in the topology, identify if it is a:

Kubernetes pod / service / stateful set

VM (Linux/Windows)

AWS Resource (EC2, EKS, RDS, Lambda, S3, etc.)

Azure Resource (VM, VMSS, WebApp, AKS, etc.)

GCP Resource (Compute Engine, Cloud Storage, Cloud SQL, GKE, etc.)

3. CROSS-REFERENCE FAILURE MODES
Use EXACT categories below. For each entity:

Select valid chaos scenarios from reference table only

Generate failure propagation mapping:

Root Failure (first break)

Propagation Impact (cascading failures)

Blast Radius (spread extent)

Recovery Path / Mitigation

4. GENERATE CHAOS PLAN (4 scenarios)
Template for each scenario:

Entity (VM, Pod, etc.) - with specific IPs/FQDNs/pod names

Failure Type (from reference only)

Hypothesis (expected breakage)

Steady State Metrics (latency, throughput, error rate baseline)

Failure Injection Method (how to induce chaos)

Propagation Path (downstream dependency failures)

Blast Radius Analysis (impact scope)

Rollback / Abort Criteria

Observability Hooks (metrics, logs, alerts to monitor)

Resiliency Goal (test validation purpose)

OUTPUT FORMAT (3 parts):

Textual topology view - all services + communication flows

Cross-reference table - entities → possible failure scenarios

Full chaos plan - 4 scenarios covering component failures, stress conditions, network conditions, internal/external dependencies

CRITICAL RULES:

Use only provided reference scenarios (no inventions)

Capture ALL internal & external communications

Identify reliability/security hotspots + recommendations

Show multi-level propagation (entity failure → cascading impact)

Treat as deliverable for SRE + Risk Review Board

Include configured replica counts and scale-down targets

Be specific: Include IPs, FQDNs, pod names where available

FAILURE REFERENCE TABLES (Use Only These):

Treat this as if delivering to an SRE + Risk Review Board.the cross referecne info is: #	Category	VM - Linux	VM - Windows
1	Component Failures	Loss of VM	Loss of VM
2		Loss of interfacing system	Loss of interfacing system
3		Loss of DNS	Loss of DNS
4		Loss of LDAP	Loss of LDAP
5		Application process terminated	Application process terminated
6		Application process hung
7		Loss of DB connectivity
8		Loss of MQ connectivity
9		Loss of filesystem
10		Filesystem corruption
11		Kernel Panic
12	Stress Conditions	CPU starvation	CPU starvation
13		Memory starvation	Memory starvation
14		High I/O	High I/O
15		Filesystem full	Drive full
16		Loss of filesystem
17	Network Conditions	Network latency (ingress/egress)
18		Packet loss
19		Packet corruption
20		Packet duplication
21	User related	User id locked	User id locked
22		User id expired	Password change
23	Internal failures	Time drift	Time drift
24		Certificate expiry
25	Batch related	Zero byte file
26		File format changed
27		File binary corrupt
28		Duplicate job run (idempotency)
29		File text removal (header/trailer)	  for PaaS: Category	Kubernetes / OpenShift
Component Failures	Loss of pods
	Remove service endpoint
	Cordon node
	Delete node
	Delete service
	Delete replicate set
	Remove stateful set
Stress Conditions	High CPU on pods
	High Memory on pods
	High I/O on pods
	Filesystem full on pods
	Scale down deployments/pods
	Scale down replica sets
	Scale down stateful sets
Network conditions	Block Traffic (ALL) - Namespace
	Block Traffic (Target) - Namespace
	Remove network policy
	Block Traffic (ALL) - Pod
	Block Traffic (Target) - Pod , for ews: Resource Type	Category	Scenarios
EC2	Component Failures	Detach random volume
EC2	Component Failures	Restart instances
EC2	Component Failures	Stop instance/instances
EC2	Component Failures	Terminate instance/instances
EC2	Component Failures	Component failures - loss of connectivity to interfacing system
EC2	Component Failures	Component failures - terminate application process
EC2	Component Failures	Component failures - hang application process
EC2	Stress Conditions	High CPU
EC2	Stress Conditions	High Memory
EC2	Stress Conditions	High IO
EC2	Stress Conditions	Disk full
EC2	Network Conditions	Latency (ingress/egress)
EC2	Network Conditions	Packet loss (ingress/egress)
EC2	Network Conditions	Packet corruption (ingress/egress)
EC2	Network Conditions	Packet duplication (ingress/egress)
EC2	Internal Failures	Lock user
EC2	Internal Failures	Expire user
EC2	Internal Failures	Time drift
EC2	Internal Failures	Certificate expiry
EKS	Component Failures	Delete cluster
RDS	Component Failures	Delete DB cluster
RDS	Component Failures	Delete DB cluster endpoint
RDS	Component Failures	Delete DB instance
RDS	Component Failures	Failover DB cluster
RDS	Component Failures	Reboot DB instance
RDS	Component Failures	Stop DB cluster
RDS	Component Failures	Stop DB instance
RDS	Stress Conditions	Block tables
Lambda	Component Failures	Delete event source mapping
Lambda	Component Failures	Delete function concurrency
Lambda	Stress Conditions	Change (put) function timeout
Lambda	Component Failures	Toggle event source mapping
Lambda	Stress Conditions	Change (put) function memory size
ASG	Network Conditions	Change subnets
ASG	Component Failures	Detach random volume
ASG	Component Failures	Detach random instances
ASG	Component Failures	Suspend processes
ASG	Component Failures	Terminate random instances
S3	Component Failures	Delete objects
S3	Component Failures	Toggle versions
Lambda	Component Failures	Memory Failure
DDB	Stress Conditions	Read Write Capacity
ECS	Component Failures	delete_cluster
ECS	Component Failures	delete_service
ECS	Component Failures	deregister_container_instance
ECS	Component Failures	stop_random_tasks
ECS	Component Failures	stop_task
ECS	Component Failures	untag_resource
ECS	Stress Conditions	Reduce number of tasks
Network	Component Failures	disassociate_vpc_from_zone
Elastic Cache	Component Failures	delete_cache_clusters
Elastic Cache	Component Failures	delete_replication_groups
Elastic Cache	Component Failures	reboot_cache_clusters
Elastic Cache	Component Failures	test_failover
ELBv2	Component Failures	delete_load_balancer
ELBv2	Component Failures	deregister_target
EMR	Component Failures	modify_cluster
EMR	Component Failures	modify_instance_fleet
EMR	Component Failures	modify_instance_groups_instance_count
EMR	Component Failures	modify_instance_groups_shrink_policy
IAM	Component Failures	detach_role_policy
EKS	Component Failures	Loss of pods
EKS	Component Failures	Remove service endpoint
EKS	Component Failures	Cordon node
EKS	Component Failures	Delete node
EKS	Component Failures	Delete service
EKS	Component Failures	Delete replicate set
EKS	Component Failures	Remove stateful set
EKS	Stress Conditions	High CPU on pods
EKS	Stress Conditions	High Memory on pods
EKS	Stress Conditions	High I/O on pods
EKS	Stress Conditions	Filesystem full on pods
EKS	Stress Conditions	Scale down pods
EKS	Stress Conditions	Scale down replica sets
EKS	Stress Conditions	Scale down stateful sets
EKS	Network conditions	Block Traffic (ALL) - Namespace
EKS	Network conditions	Block Traffic (Target) - Namespace
EKS	Network conditions	Remove network policy
EKS	Network conditions	Block Traffic (ALL) - Pod
EKS	Network conditions	Block Traffic (Target) - Pod
for azure, Resource Type	Category	Scenarios
VM	Component Failures	Delete VM
VM	Stress Conditions	Disk full
VM	Component Failures	Restart VM
VM	Component Failures	Terminate application process
VM	Component Failures	Hang application process
VM	Stress Conditions	High CPU
VM	Stress Conditions	High Memory
VM	Stress Conditions	High IO
VM	Stress Conditions	Disk full
VM	Network Conditions	Network Latency (ingress/egress)
VM	Network Conditions	Packet loss (ingress/egress)
VM	Network Conditions	Packet corruption (ingress/egress)
VM	Network Conditions	Packet duplication (ingress/egress)
VM	Internal Failures	Lock user
VM	Internal Failures	Expire user
VM	Internal Failures	Time drift
VM	Internal Failures	Certificate expiry
VMSS	Stress Conditions	High IO
VMSS	Component Failures	Deallocate VMSS
VMSS	Component Failures	Restart VMSS
VMSS	Component Failures	Loss of VMSS
VMSS	Network Conditions	Network latency
VMSS	Stress Conditions	High CPU on VMSS instance
Webapp	Component Failures	Delete webapp
Webapp	Component Failures	Restart webapp
Webapp	Component Failures	Stop webapp
AKS	Component Failures	Delete node
AKS	Component Failures	Restart node
AKS	Component Failures	Stop node
AKS	Component Failures	Loss of pods
AKS	Component Failures	Remove service endpoint
AKS	Component Failures	Cordon node
AKS	Component Failures	Delete node
AKS	Component Failures	Delete service
AKS	Component Failures	Delete replicate set
AKS	Component Failures	Remove stateful set
AKS	Stress Conditions	High CPU on pods
AKS	Stress Conditions	High Memory on pods
AKS	Stress Conditions	High I/O on pods
AKS	Stress Conditions	Filesystem full on pods
AKS	Stress Conditions	Scale down pods
AKS	Stress Conditions	Scale down replica sets
AKS	Stress Conditions	Scale down stateful sets
AKS	Network conditions	Block Traffic (ALL) - Namespace
AKS	Network conditions	Block Traffic (Target) - Namespace
AKS	Network conditions	Remove network policy
AKS	Network conditions	Block Traffic (ALL) - Pod
AKS	Network conditions	Block Traffic (Target) - Pod
for GCp Resource Type		Scenarios
Compute Engine		Terminate VM
		Detach storage
		Detach random storage
		Stop VM
		Restart VM
		Loss of interfacing system
		Loss of DNS
		Loss of LDAP
		Application process terminated
		Application process hung
		Loss of DB connectivity
		Loss of MQ connectivity
		Loss of filesystem
		Filesystem corruption
		Kernel Panic
		CPU starvation
		Memory starvation
		High I/O
		Filesystem full
		Loss of filesystem
		Network latency (ingress/egress)
		Packet loss
		Packet corruption
		Packet duplication
		User id locked
		User id expired
		Time drift
		Certificate expiry
		Zero byte file
		File format changed
		File binary corrupt
		Duplicate job run (idempotency)
		File text removal (header/trailer)
Cloud Storage		Delete object
		Toggle version
Cloud SQL		Stop Sql
		Terminate Sql
		Stop Sql
		Reboot Sql
		enable_replication
		Failover
GKE	Networking	namespace_network_block_full
		namespace_network_block_on_target
		remove_reinstate_networkpolicy
	Node	cordon_node
		delete_node
	Pod	filesystem_full_pods
		high_cpu_on_pod
		high_io_on_pod
		high_memory_on_pod
		loss_of_pods
		pod_network_block_full
		pod_network_block_on_target
		scale_down_pods
	Replica set	delete_replica_set
		scale_down_replica_set
	Service	delete_service
	Statefulset	remove_reinstate_statefulset
		scale_down_stateful_set

TARGET OUTPUT: Under 4096 tokens, comprehensive, actionable for SRE team. Don't need to give unnecessary information, stick to the plan"""
        return prompt
